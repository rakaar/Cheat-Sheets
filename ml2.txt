model representation
	
	data(training set) ->	learning algo	->	function(hypothesis)
													[this function will fetch u the required output]

	function is in the form of ax+b then its a  univariate(single variable) linear regression(as we know) model

Gradient descent practical aspects
	scaling of data 
		x1: 0 to 2000
		x2: 0 to 5
	scale both of them 
		x1/2000
		x2/5

Feature scaling:
	take xi divide it by its range
Mean normalisation:
	subtract from its average and divide by its range

Learning Rate
	should not be too small - slow convergence
	should not be too large - may also diverge
best way to test whether gradient descent is working or not:
	draw a plot and for each iteration J(theta) should decrease
	and at a certain ending stage : it becomes stagnant

propsing own new features
	if some hypothesis is like a+ b*l + c*b
	l is length and b is breadth
	you can produce some thing like A = l*b
	hypo would become a + b*A	

Polynomial regression
apart from linear regression , so u can fit any polynomial to fit to your data
it means that the hypothesis can be either quad , or cubic or also fractional power as per ur choice
hence it would be cause polynomial regression rather than linear regression..
	ya but things like no. of minima will also change